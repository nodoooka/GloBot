import json
import sqlite3
import os
import sys
import asyncio
import time
from pathlib import Path
from datetime import datetime

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ç³»ç»Ÿè·¯å¾„
sys.path.append(str(Path(__file__).resolve().parent.parent))
from common.config_loader import settings

from Bot_Crawler.media_downloader import download_media  

# ğŸŒŸ GloBot åŠ¨æ€è·¯å¾„
FACTORY_DIR = Path(os.getenv("LOCAL_DATA_DIR", f"./GloBot_Data/{settings.targets.group_name}"))
DB_PATH = FACTORY_DIR / "processed_tweets.db"

def init_db():
    FACTORY_DIR.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS tweets (
            tweet_id TEXT PRIMARY KEY,
            author TEXT,
            extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    conn.commit()
    return conn

def find_tweets(obj):
    if isinstance(obj, dict):
        # ğŸ’£ æ ¸å¿ƒä¿®å¤ï¼šå¿…é¡»å¸¦æœ‰å…¨æ™¯æ­£æ–‡ï¼Œæˆ–è€…æ˜¾å¼å£°æ˜ç±»å‹æ˜¯ Tweetï¼Œå½»åº•æ’é™¤ User å¯¹è±¡
        is_tweet = 'Tweet' in str(obj.get('__typename', '')) or 'full_text' in obj.get('legacy', {})
        
        if is_tweet and 'legacy' in obj and 'rest_id' in obj and 'core' in obj:
            yield obj
            
        for k, v in obj.items():
            yield from find_tweets(v)
    elif isinstance(obj, list):
        for item in obj:
            yield from find_tweets(item)

def find_key(obj, target_key):
    if isinstance(obj, dict):
        if target_key in obj:
            return obj[target_key]
        for v in obj.values():
            res = find_key(v, target_key)
            if res is not None:
                return res
    elif isinstance(obj, list):
        for item in obj:
            res = find_key(item, target_key)
            if res is not None:
                return res
    return None

async def parse_timeline_json(json_file_path: Path) -> list:
    """è§£æ JSON çŸ¿çŸ³ï¼Œå¹¶è¿”å›ç»“æ„åŒ–çš„æ–°æ¨æ–‡åˆ—è¡¨ç»™æ€»æ§ä¸­å¿ƒ"""
    print(f"ğŸ”¬ æ­£åœ¨åŒ–éªŒçŸ¿çŸ³: {json_file_path.name}")
    
    with open(json_file_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    conn = init_db()
    cursor = conn.cursor()
    target_accounts = [acc.lower() for acc in settings.targets.x_accounts]
    
    parsed_new_tweets = []

    for tweet_node in find_tweets(data):
        tweet_id = tweet_node.get('rest_id')
        legacy = tweet_node.get('legacy', {})
        raw_screen_name = find_key(tweet_node.get('core', {}), 'screen_name')
        author_screen_name = str(raw_screen_name).lower() if raw_screen_name else ''
        
        if author_screen_name not in target_accounts:
            continue
        if 'retweeted_status_result' in legacy and 'is_quote_status' not in legacy:
            continue
            
        cursor.execute("SELECT 1 FROM tweets WHERE tweet_id = ?", (tweet_id,))
        if cursor.fetchone():
            continue

        # ==========================================
        # ğŸ”ª æš´åŠ›æå–æ–‡æœ¬é€»è¾‘ (å½»åº•æ ¹æ²»é•¿æ¨æ–‡ä¸å¼•ç”¨æ¨æ–‡ä¸¢å¤±é—®é¢˜)
        # ==========================================
        full_text = legacy.get('full_text', '')
        
        # 1. å…¼å®¹æ¨ç‰¹è“Vé•¿æ¨æ–‡ (Note Tweet)
        try:
            if 'note_tweet_results' in tweet_node:
                note_text = tweet_node['note_tweet_results'].get('result', {}).get('text', '')
                if note_text:
                    full_text = note_text
            elif 'note_tweet' in tweet_node: # æŸäº›æ—§ç‰ˆ GraphQL æ ¼å¼å…¼å®¹
                note_text = tweet_node['note_tweet'].get('note_tweet_results', {}).get('result', {}).get('text', '')
                if note_text:
                    full_text = note_text
        except Exception:
            pass
            
        # 2. å…¼å®¹å¼•ç”¨æ¨æ–‡ (Quote Tweet)
        try:
            if 'quoted_status_result' in tweet_node:
                quote_legacy = tweet_node['quoted_status_result']['result']['legacy']
                quote_text = quote_legacy.get('full_text', '')
                if quote_text:
                    full_text = full_text + f"\n\nã€å¼•ç”¨å†…å®¹ã€‘:\n{quote_text}"
        except Exception:
            pass
            
        full_text = full_text.strip()
        # ==========================================

        media_files = legacy.get('extended_entities', {}).get('media', [])
        
        # è§£ææ—¶é—´æˆ³
        raw_created_at = legacy.get('created_at', '')
        try:
            dt = datetime.strptime(raw_created_at, "%a %b %d %H:%M:%S %z %Y")
            timestamp_sec = int(dt.timestamp())
        except:
            timestamp_sec = int(time.time()) 
        
        print(f"\nğŸŒŸ [æ–°åŠ¨æ€å‘ç°] ä½œè€…: @{author_screen_name} (ID: {tweet_id})")
        
        member_media_dir = FACTORY_DIR / "media" / author_screen_name
        local_media_paths = []
        
        img_count = 1
        for media in media_files:
            if media['type'] == 'photo':
                orig_url = media['media_url_https'] + "?name=orig"
                filename = f"{tweet_id}_img{img_count}.jpg"
                if await download_media(orig_url, member_media_dir, filename):
                    local_media_paths.append(str(member_media_dir / filename))
                img_count += 1
                
            elif media['type'] in ['video', 'animated_gif']:
                variants = media.get('video_info', {}).get('variants', [])
                mp4_variants = [v for v in variants if v.get('content_type') == 'video/mp4' and 'bitrate' in v]
                if mp4_variants:
                    best_video = sorted(mp4_variants, key=lambda x: x['bitrate'], reverse=True)[0]
                    vid_url = best_video['url']
                    filename = f"{tweet_id}_video.mp4"
                    if await download_media(vid_url, member_media_dir, filename):
                        local_media_paths.append(str(member_media_dir / filename))

        cursor.execute("INSERT INTO tweets (tweet_id, author) VALUES (?, ?)", (tweet_id, author_screen_name))
        conn.commit()

        parsed_new_tweets.append({
            'id': tweet_id,
            'author': author_screen_name,
            'text': full_text,
            'media': local_media_paths,
            'timestamp': timestamp_sec
        })

    conn.close()
    
    if not parsed_new_tweets:
        print("ğŸ’¤ æ²¡æœ‰å‘ç°æ–°çš„ç›‘æ§å¯¹è±¡åŠ¨æ€ï¼Œæˆ–å…¨æ˜¯æ—§æ•°æ®ã€‚")
    else:
        print(f"\nâœ… æçº¯ä¸ä¸‹è½½å…¨éƒ¨å®Œæˆï¼å…±æå– {len(parsed_new_tweets)} æ¡å…¨æ–°åŠ¨æ€ã€‚")
        
    return parsed_new_tweets

if __name__ == "__main__":
    raw_dir = FACTORY_DIR / "timeline_raw"
    json_files = list(raw_dir.glob("*.json"))
    if not json_files:
        print("âŒ æ–‡ä»¶å¤¹é‡Œç©ºç©ºå¦‚ä¹Ÿï¼Œæ²¡æœ‰æ‰¾åˆ°ä»»ä½• JSON çŸ¿çŸ³ï¼")
    else:
        latest_json = max(json_files, key=os.path.getmtime)
        print(f"ğŸ¤– [è‡ªåŠ¨å¯»æ•Œ] å‘ç°æœ€æ–°æŠ“å–çš„æ•°æ®åŒ…ï¼š{latest_json.name}\n")
        res = asyncio.run(parse_timeline_json(latest_json))
        print(f"\næµ‹è¯•è¿”å›æ•°æ®é¢„è§ˆ: {json.dumps(res, ensure_ascii=False, indent=2)}")