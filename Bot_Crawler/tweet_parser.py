import json
import sqlite3
import os
import sys
import asyncio
import time
from pathlib import Path
from datetime import datetime

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ç³»ç»Ÿè·¯å¾„
sys.path.append(str(Path(__file__).resolve().parent.parent))
from common.config_loader import settings

# ğŸŒŸ å®Œç¾é€‚é… Bot_Crawler
from Bot_Crawler.media_downloader import download_media  

# ğŸŒŸ GloBot åŠ¨æ€è·¯å¾„
FACTORY_DIR = Path(os.getenv("LOCAL_DATA_DIR", f"./GloBot_Data/{settings.targets.group_name}"))
DB_PATH = FACTORY_DIR / "processed_tweets.db"

def init_db():
    FACTORY_DIR.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS tweets (
            tweet_id TEXT PRIMARY KEY,
            author TEXT,
            extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    conn.commit()
    return conn

def find_tweets(obj):
    if isinstance(obj, dict):
        if 'legacy' in obj and 'rest_id' in obj and 'core' in obj:
            yield obj
        for k, v in obj.items():
            yield from find_tweets(v)
    elif isinstance(obj, list):
        for item in obj:
            yield from find_tweets(item)

def find_key(obj, target_key):
    if isinstance(obj, dict):
        if target_key in obj:
            return obj[target_key]
        for v in obj.values():
            res = find_key(v, target_key)
            if res is not None:
                return res
    elif isinstance(obj, list):
        for item in obj:
            res = find_key(item, target_key)
            if res is not None:
                return res
    return None

async def parse_timeline_json(json_file_path: Path) -> list:
    """è§£æ JSON çŸ¿çŸ³ï¼Œå¹¶è¿”å›ç»“æ„åŒ–çš„æ–°æ¨æ–‡åˆ—è¡¨ç»™æ€»æ§ä¸­å¿ƒ"""
    print(f"ğŸ”¬ æ­£åœ¨åŒ–éªŒçŸ¿çŸ³: {json_file_path.name}")
    
    with open(json_file_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    conn = init_db()
    cursor = conn.cursor()
    target_accounts = [acc.lower() for acc in settings.targets.x_accounts]
    
    parsed_new_tweets = [] # ğŸŒŸ æ–°å¢ï¼šç”¨äºæ”¶é›†è¦è¿”å›çš„æ–°æ¨æ–‡æ•°æ®

    for tweet_node in find_tweets(data):
        tweet_id = tweet_node.get('rest_id')
        legacy = tweet_node.get('legacy', {})
        raw_screen_name = find_key(tweet_node.get('core', {}), 'screen_name')
        author_screen_name = str(raw_screen_name).lower() if raw_screen_name else ''
        
        if author_screen_name not in target_accounts:
            continue
        if 'retweeted_status_result' in legacy and 'is_quote_status' not in legacy:
            continue
            
        cursor.execute("SELECT 1 FROM tweets WHERE tweet_id = ?", (tweet_id,))
        if cursor.fetchone():
            continue

        full_text = legacy.get('full_text', '')
        media_files = legacy.get('extended_entities', {}).get('media', [])
        
        # ğŸŒŸ æ–°å¢ï¼šè§£ææ¨ç‰¹åŸå§‹æ—¶é—´æˆ³
        raw_created_at = legacy.get('created_at', '')
        try:
            # æ¨ç‰¹æ ¼å¼: "Wed Oct 10 20:19:24 +0000 2018"
            dt = datetime.strptime(raw_created_at, "%a %b %d %H:%M:%S %z %Y")
            timestamp_sec = int(dt.timestamp())
        except:
            timestamp_sec = int(time.time()) # è§£æå¤±è´¥å…œåº•ä¸ºå½“å‰æ—¶é—´
        
        print(f"\nğŸŒŸ [æ–°åŠ¨æ€å‘ç°] ä½œè€…: @{author_screen_name} (ID: {tweet_id})")
        
        member_media_dir = FACTORY_DIR / "media" / author_screen_name
        local_media_paths = [] # ğŸŒŸ æ–°å¢ï¼šæ”¶é›†ä¸‹è½½åçš„æœ¬åœ°è·¯å¾„
        
        img_count = 1
        for media in media_files:
            if media['type'] == 'photo':
                orig_url = media['media_url_https'] + "?name=orig"
                filename = f"{tweet_id}_img{img_count}.jpg"
                if await download_media(orig_url, member_media_dir, filename):
                    local_media_paths.append(str(member_media_dir / filename))
                img_count += 1
                
            elif media['type'] in ['video', 'animated_gif']:
                variants = media.get('video_info', {}).get('variants', [])
                mp4_variants = [v for v in variants if v.get('content_type') == 'video/mp4' and 'bitrate' in v]
                if mp4_variants:
                    best_video = sorted(mp4_variants, key=lambda x: x['bitrate'], reverse=True)[0]
                    vid_url = best_video['url']
                    filename = f"{tweet_id}_video.mp4"
                    if await download_media(vid_url, member_media_dir, filename):
                        local_media_paths.append(str(member_media_dir / filename))

        cursor.execute("INSERT INTO tweets (tweet_id, author) VALUES (?, ?)", (tweet_id, author_screen_name))
        conn.commit()

        # ğŸŒŸ æ–°å¢ï¼šå°†ç»„è£…å¥½çš„æ•°æ®å¡å…¥åˆ—è¡¨
        parsed_new_tweets.append({
            'id': tweet_id,
            'author': author_screen_name,
            'text': full_text,
            'media': local_media_paths,
            'timestamp': timestamp_sec
        })

    conn.close()
    
    if not parsed_new_tweets:
        print("ğŸ’¤ æ²¡æœ‰å‘ç°æ–°çš„ç›‘æ§å¯¹è±¡åŠ¨æ€ï¼Œæˆ–å…¨æ˜¯æ—§æ•°æ®ã€‚")
    else:
        print(f"\nâœ… æçº¯ä¸ä¸‹è½½å…¨éƒ¨å®Œæˆï¼å…±æå– {len(parsed_new_tweets)} æ¡å…¨æ–°åŠ¨æ€ã€‚")
        
    return parsed_new_tweets # ğŸŒŸ æ ¸å¿ƒï¼šæŠŠæ•°æ®è¿˜ç»™æ€»æ§å°ï¼

if __name__ == "__main__":
    raw_dir = FACTORY_DIR / "timeline_raw"
    json_files = list(raw_dir.glob("*.json"))
    if not json_files:
        print("âŒ æ–‡ä»¶å¤¹é‡Œç©ºç©ºå¦‚ä¹Ÿï¼Œæ²¡æœ‰æ‰¾åˆ°ä»»ä½• JSON çŸ¿çŸ³ï¼")
    else:
        latest_json = max(json_files, key=os.path.getmtime)
        print(f"ğŸ¤– [è‡ªåŠ¨å¯»æ•Œ] å‘ç°æœ€æ–°æŠ“å–çš„æ•°æ®åŒ…ï¼š{latest_json.name}\n")
        res = asyncio.run(parse_timeline_json(latest_json))
        print(f"\næµ‹è¯•è¿”å›æ•°æ®é¢„è§ˆ: {json.dumps(res, ensure_ascii=False, indent=2)}")