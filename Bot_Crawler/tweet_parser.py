import json
import sqlite3
import os
import sys
import asyncio
import time
import re
from pathlib import Path
from datetime import datetime

sys.path.append(str(Path(__file__).resolve().parent.parent))
from common.config_loader import settings
from Bot_Crawler.media_downloader import download_media  

FACTORY_DIR = Path(os.getenv("LOCAL_DATA_DIR", f"./GloBot_Data/{settings.targets.group_name}"))
DB_PATH = FACTORY_DIR / "processed_tweets.db"

def init_db():
    FACTORY_DIR.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS tweets (
            tweet_id TEXT PRIMARY KEY,
            author TEXT,
            extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    conn.commit()
    return conn

def find_tweets(obj):
    if isinstance(obj, dict):
        is_tweet = 'Tweet' in str(obj.get('__typename', '')) or 'full_text' in obj.get('legacy', {})
        if is_tweet and 'legacy' in obj and 'rest_id' in obj and 'core' in obj:
            yield obj
        for k, v in obj.items():
            yield from find_tweets(v)
    elif isinstance(obj, list):
        for item in obj:
            yield from find_tweets(item)

def find_key(obj, target_key):
    if isinstance(obj, dict):
        if target_key in obj: return obj[target_key]
        for v in obj.values():
            res = find_key(v, target_key)
            if res is not None: return res
    elif isinstance(obj, list):
        for item in obj:
            res = find_key(item, target_key)
            if res is not None: return res
    return None

def extract_tweet_node(node):
    tweet_id = str(node.get('rest_id', ''))
    legacy = node.get('legacy', {})
    raw_screen_name = find_key(node.get('core', {}), 'screen_name')
    author_screen_name = str(raw_screen_name).lower() if raw_screen_name else ''
    
    # ğŸ‘‡ æ–°å¢ï¼šç²¾å‡†æå–æ¨ç‰¹è´¦å·çš„çœŸå®æ˜¾ç¤ºåç§°ï¼ˆDisplay Nameï¼‰
    raw_display_name = find_key(node.get('core', {}), 'name')
    author_display_name = str(raw_display_name) if raw_display_name else f"@{author_screen_name}"    
    # ğŸ‘‡ æ–°å¢ï¼šç²¾å‡†æå–åº•å±‚çš„è¯„è®ºå›å¤å¯¹è±¡å±æ€§
    raw_reply_name = legacy.get('in_reply_to_screen_name')
    in_reply_to_screen_name = str(raw_reply_name).lower() if raw_reply_name else None
    in_reply_to_status_id_str = legacy.get('in_reply_to_status_id_str')
    
    full_text = legacy.get('full_text', '')
    try:
        if 'note_tweet_results' in node:
            nt = node['note_tweet_results'].get('result', {}).get('text', '')
            if nt: full_text = nt
        elif 'note_tweet' in node: 
            nt = node['note_tweet'].get('note_tweet_results', {}).get('result', {}).get('text', '')
            if nt: full_text = nt
    except: pass
        
    # 1. å¹²æ‰æ¨ç‰¹è‡ªå¸¦çš„ t.co çŸ­é“¾
    full_text = re.sub(r'https?://t\.co/\w+', '', full_text).strip()
    
    # 2. ğŸš¨ ç²¾å‡†å¤–ç§‘æ‰‹æœ¯ï¼šåªåœ¨â€œè¯„è®ºå›å¤â€çš„åœºæ™¯ä¸‹ï¼Œåˆ‡é™¤æ¨ç‰¹åº•å±‚å¼ºåˆ¶å¡å…¥çš„ @è´¦å·æ ‡ç­¾ï¼
    # è¿™æ ·å°±èƒ½å®Œç¾ä¿æŠ¤æ™®é€šæ¨æ–‡ä¸­ï¼Œå¶åƒä¸»åŠ¨è‰¾ç‰¹åˆ«äººï¼ˆå¦‚æ‘„å½±å¸ˆã€å®˜æ–¹å·ï¼‰çš„æ­£å¸¸äº¤äº’ã€‚
    if legacy.get('in_reply_to_status_id_str'):
        full_text = re.sub(r'^(@\w+\s*)+', '', full_text).strip()

    media_files = legacy.get('extended_entities', {}).get('media', [])
    
    raw_created_at = legacy.get('created_at', '')
    try:
        dt = datetime.strptime(raw_created_at, "%a %b %d %H:%M:%S %z %Y")
        timestamp_sec = int(dt.timestamp())
    except:
        timestamp_sec = int(time.time()) 
        
    return {
        'id': tweet_id,
        'author': author_screen_name,
        'author_display_name': author_display_name, # ğŸ‘‡ æ–°å¢è¿™ä¸€è¡Œ
        'text': full_text,
        'media_files_raw': media_files,
        'timestamp': timestamp_sec,
        'in_reply_to_screen_name': in_reply_to_screen_name,
        'in_reply_to_status_id_str': in_reply_to_status_id_str,
        'raw_node': node
    }

async def parse_timeline_json(json_file_path: Path) -> list:
    print(f"ğŸ”¬ æ­£åœ¨åŒ–éªŒçŸ¿çŸ³: {json_file_path.name}")
    with open(json_file_path, "r", encoding="utf-8") as f: data = json.load(f)

    conn = init_db()
    cursor = conn.cursor()
    target_accounts = [acc.lower() for acc in settings.targets.x_accounts]
    parsed_new_tweets = []

    for tweet_node in find_tweets(data):
        target_info = extract_tweet_node(tweet_node)
        
        if target_info['author'] not in target_accounts: continue
        if 'retweeted_status_result' in tweet_node.get('legacy', {}): continue
            
        # ==========================================
        # ğŸš¨ ç—›ç‚¹ä¿®å¤ï¼šè¯„è®ºåŒºå›å¤çš„è¿‡æ»¤ä¸å¥—å¨ƒ
        # ==========================================
        reply_to_user = target_info.get('in_reply_to_screen_name')
        if reply_to_user:
            if reply_to_user not in target_accounts:
                continue # è§„åˆ™ 1ï¼šå½»åº•å¿½ç•¥æˆå‘˜å¯¹å¤–éƒ¨è´¦å·/è·¯äººç²‰ä¸çš„å›å¤
            else:
                # è§„åˆ™ 2ï¼šæˆå‘˜é—´äº’ç›¸å›å¤ï¼Œä¼ªè£…æˆå¼•ç”¨è½¬å‘ï¼Œè§¦å‘ B ç«™å¥—å¨ƒï¼
                target_info['quoted_tweet_id'] = target_info.get('in_reply_to_status_id_str')

        cursor.execute("SELECT 1 FROM tweets WHERE tweet_id = ?", (target_info['id'],))
        if cursor.fetchone(): continue

        quote_chain = []
        curr_node = tweet_node
        while True:
            q_res = curr_node.get('quoted_status_result', {}).get('result', {})
            if q_res.get('__typename') == 'TweetWithVisibilityResults':
                q_res = q_res.get('tweet', {})
                
            if not q_res or 'legacy' not in q_res:
                break
                
            q_info = extract_tweet_node(q_res)
            
            # å¦‚æœæ˜¯å¸¦è¯„è®ºè½¬å‘ï¼Œè®°å½•ç›´æ¥çˆ¶èŠ‚ç‚¹ ID (ç”±äºå¼•ç”¨çº§åˆ«å¾€å¾€æ¯”æ™®é€šçš„è¯„è®ºå±•ç¤ºä¼˜å…ˆçº§é«˜ï¼Œæ‰€ä»¥è¦†ç›–å›¢å†…å›å¤)
            if not quote_chain:
                target_info['quoted_tweet_id'] = q_info['id']
                target_info['quoted_text'] = q_info['text']
                
            quote_chain.insert(0, q_info)
            curr_node = q_res

        # ğŸ–¼ï¸ ä¸ºé“¾æ¡ä¸Šçš„æ¯ä¸€ä¸ªèŠ‚ç‚¹ä¸‹è½½åª’ä½“æ–‡ä»¶å¹¶æå– ALT
        all_nodes = quote_chain + [target_info]
        for node in all_nodes:
            member_media_dir = FACTORY_DIR / "media" / node['author']
            member_media_dir.mkdir(parents=True, exist_ok=True)
            local_media = []
            img_count = 1
            alt_texts = [] 
            
            for media in node['media_files_raw']:
                if media['type'] == 'photo':
                    orig_url = media['media_url_https'] + "?name=orig"
                    filename = f"{node['id']}_img{img_count}.jpg"
                    if await download_media(orig_url, member_media_dir, filename):
                        local_media.append(str(member_media_dir / filename))
                    
                    alt = media.get('ext_alt_text')
                    if alt:
                        alt_texts.append(f"ã€å›¾{img_count}é™„è¨€ã€‘\n{alt.strip()}")
                        
                    img_count += 1
                elif media['type'] in ['video', 'animated_gif']:
                    variants = media.get('video_info', {}).get('variants', [])
                    mp4_variants = [v for v in variants if v.get('content_type') == 'video/mp4' and 'bitrate' in v]
                    if mp4_variants:
                        best_video = sorted(mp4_variants, key=lambda x: x['bitrate'], reverse=True)[0]
                        vid_url = best_video['url']
                        filename = f"{node['id']}_video.mp4"
                        if await download_media(vid_url, member_media_dir, filename):
                            local_media.append(str(member_media_dir / filename))
            
            node['media'] = local_media
            
            if alt_texts:
                node['text'] += "\n\n" + "\n\n".join(alt_texts)

        cursor.execute("INSERT INTO tweets (tweet_id, author) VALUES (?, ?)", (target_info['id'], target_info['author']))
        conn.commit()

        target_info['quote_chain'] = quote_chain
        parsed_new_tweets.append(target_info)

    conn.close()
    if parsed_new_tweets: print(f"\nâœ… æçº¯ä¸ä¸‹è½½å…¨éƒ¨å®Œæˆï¼å…±æå– {len(parsed_new_tweets)} æ¡å…¨æ–°åŠ¨æ€ã€‚")
    return parsed_new_tweets

if __name__ == "__main__":
    raw_dir = FACTORY_DIR / "timeline_raw"
    json_files = list(raw_dir.glob("*.json"))
    if json_files:
        latest_json = max(json_files, key=os.path.getmtime)
        res = asyncio.run(parse_timeline_json(latest_json))
        print(f"\næµ‹è¯•è¿”å›æ•°æ®é¢„è§ˆ: {json.dumps(res, ensure_ascii=False, indent=2)}")