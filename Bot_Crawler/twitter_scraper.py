import asyncio
import json
import os
import sys
import logging
import random
from datetime import datetime
from pathlib import Path

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ç³»ç»Ÿè·¯å¾„
sys.path.append(str(Path(__file__).resolve().parent.parent))

from playwright.async_api import async_playwright, Response
from common.config_loader import settings

# ==========================================
# 1. åŸºç¡€é…ç½®
# ==========================================
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

AUTH_FILE = Path(__file__).resolve().parent.parent / "auth_store" / "twitter_auth.json"

# ğŸŒŸ GloBot åŠ¨æ€è·¯å¾„ï¼šæ ¹æ®é…ç½®è¡¨çš„ group_name è‡ªåŠ¨å»ºç«‹å¯¹åº”å›¢ä½“çš„æ–‡ä»¶å¤¹ï¼
DATA_DIR = Path(os.getenv("LOCAL_DATA_DIR", f"./GloBot_Data/{settings.targets.group_name}")) / "timeline_raw"
DATA_DIR.mkdir(parents=True, exist_ok=True)

# ==========================================
# 2. æ ¸å¿ƒæ‹¦æˆªå™¨ï¼šã€ä¸¥æ ¼æ‹‰é»‘â€œä¸ºä½ æ¨èâ€ï¼Œåªæˆªè·â€œæ­£åœ¨å…³æ³¨â€ã€‘
# ==========================================
async def handle_response(response: Response):
    """ç²¾å‡†æ‹¦æˆª HomeLatestTimeline (å³'æ­£åœ¨å…³æ³¨'çš„çº¯å‡€æ—¶é—´çº¿)"""
    if "graphql" in response.url and "HomeLatestTimeline" in response.url:
        try:
            json_data = await response.json()
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = DATA_DIR / f"timeline_following_{timestamp}.json"
            
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"ğŸ¯ æˆåŠŸæˆªè·çº¯å‡€ç‰ˆã€æ­£åœ¨å…³æ³¨ã€‘ä¿¡æ¯æµï¼å·²ä¿å­˜è‡³ {save_path.name}")
        except Exception as e:
            logger.error(f"è§£ææ—¶é—´çº¿ GraphQL å¤±è´¥: {e}")

# ==========================================
# 3. å•æ¬¡å—…æ¢ä»»åŠ¡ (æ¨¡æ‹ŸçœŸäººç‚¹å‡»åˆ‡æ¢ Tab)
# ==========================================
async def fetch_timeline():
    logger.info("ğŸš€ å¼€å§‹æ½œå…¥ X ä¸»é¡µæå–æœ€æ–°åŠ¨æ€...")
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,  
            args=[
                "--disable-dev-shm-usage",
                "--disable-gpu",
                "--no-sandbox",
                "--blink-settings=imagesEnabled=false", 
                "--js-flags='--max-old-space-size=512'"
            ]
        )
        
        context = await browser.new_context(
            storage_state=AUTH_FILE,
            viewport={'width': 1280, 'height': 800},
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
        )
        
        page = await context.new_page()
        page.on("response", handle_response)
        
        try:
            await page.goto("https://x.com/home", timeout=60000)
            
            logger.info("ğŸ–±ï¸ æ­£åœ¨å¼ºåˆ¶åˆ‡æ¢åˆ°ã€æ­£åœ¨å…³æ³¨ã€‘(Following) é¡µé¢...")
            await page.wait_for_selector('[role="tab"]', timeout=15000)
            tabs = page.locator('[role="tab"]')
            
            if await tabs.count() >= 2:
                await tabs.nth(1).click()
            else:
                logger.warning("âš ï¸ æ ‡ç­¾é¡µè·å–å¼‚å¸¸ï¼Œæ¨ç‰¹UIå¯èƒ½å‘ç”Ÿå˜åŒ–ã€‚")
            
            logger.info("â³ æ­£åœ¨ç›‘å¬åº•å±‚çº¯å‡€æ•°æ®åŒ…...")
            await page.wait_for_timeout(8000)
            
            await page.mouse.wheel(0, 1500)
            await page.wait_for_timeout(3000)
            
        except Exception as e:
            logger.error(f"âŒ æŠ“å–è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}")
        finally:
            logger.info("ğŸ§¹ é”€æ¯æœ¬æ¬¡æµè§ˆå™¨ç¯å¢ƒï¼Œé‡Šæ”¾ç‰©ç†å†…å­˜ã€‚")
            await context.close()
            await browser.close()

# ==========================================
# 4. æ°¸åŠ¨æœºï¼šå¸¦äººæ€§åŒ–æŠ–åŠ¨ + è‡ªåŠ¨å‘¼å«æçº¯ä¸‹è½½ + é˜…åå³ç„š
# ==========================================
async def crawler_loop():
    if not AUTH_FILE.exists():
        logger.error("âŒ æ‰¾ä¸åˆ°å…å¯†é€šè¡Œè¯ auth.jsonï¼è¯·å…ˆè¿è¡Œ login_auth.py")
        return

    logger.info(f"ğŸŸ¢ GloBot ç›‘å¬çŸ©é˜µå·²å¯åŠ¨ï¼Œç›®æ ‡é›†ç¾¤: {settings.targets.group_name} ...")
    while True:
        await fetch_timeline()
        
        # ğŸŒŸ ä¿®æ”¹äº†è¿™é‡Œçš„åŒ…åå¼•ç”¨ï¼Œå®Œç¾é€‚é… Bot_Crawler
        from Bot_Crawler.tweet_parser import parse_timeline_json
        raw_dir = DATA_DIR
        json_files = list(raw_dir.glob("*.json"))
        if json_files:
            latest_json = max(json_files, key=os.path.getmtime)
            logger.info("âš™ï¸ å°†æœ€æ–°çš„ä¿¡æ¯æµç§»äº¤è‡³ã€è§£æä¸ä¸‹è½½å·¥å‚ã€‘...")
            await parse_timeline_json(latest_json)  
            
            try:
                latest_json.unlink()
                logger.info(f"ğŸ”¥ é˜…åå³ç„šï¼šå·²å½»åº•é”€æ¯åŸå§‹ JSON ({latest_json.name})ï¼Œç»ä¸æµªè´¹ç¡¬ç›˜ç©ºé—´ï¼")
            except Exception as e:
                logger.error(f"âš ï¸ é”€æ¯ JSON å¤±è´¥: {e}")
        
        sleep_time = random.randint(240, 420) 
        minutes = sleep_time // 60
        seconds = sleep_time % 60
        
        logger.info(f"ğŸ’¤ å…¨é“¾è·¯æ“ä½œå®Œæˆã€‚ä¼‘çœ  {minutes} åˆ† {seconds} ç§’ ({sleep_time}s) åå‘èµ·ä¸‹ä¸€æ¬¡å—…æ¢...\n")
        await asyncio.sleep(sleep_time)

if __name__ == "__main__":
    asyncio.run(crawler_loop())