import os
import json
import time
import logging
import asyncio
import random
import html
from Bot_Master.tg_bot import start_telegram_bot, send_tg_msg, send_tg_error, GloBotState
import traceback
from pathlib import Path
from datetime import datetime

from common.config_loader import settings
from Bot_Crawler.twitter_scraper import fetch_timeline
from Bot_Crawler.tweet_parser import parse_timeline_json
from Bot_Media.llm_translator import translate_text
from Bot_Media.media_pipeline import dispatch_media
# ğŸš¨ å¼•å…¥åˆšæ‰å†™å¥½çš„åŠ¨æ€çŒçŠ¬
from Bot_Publisher.bili_uploader import smart_publish, smart_repost, get_dynamic_id_by_bvid
from common.text_sanitizer import sanitize_for_bilibili

# ==========================================
# ğŸ”‡ å…¨å±€æ—¥å¿—é™éŸ³é…ç½® (é˜²åˆ·å±)
# ==========================================
# 1. æŠ‘åˆ¶åº•å±‚ç½‘ç»œåº“çš„å¿ƒè·³ä¸è¿æ¥æ—¥å¿—
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)

# 2. ğŸš¨ æ ¸å¿ƒä¿®å¤ï¼šå±è”½ Telegram è½®è¯¢å™¨çš„æ–­ç½‘æŠ¥é”™åˆ·å±
# Updater é‡åˆ°æ–­ç½‘ä¼šè‡ªåŠ¨é‡è¿ï¼Œå¼ºåˆ¶å°†å…¶æ—¥å¿—çº§åˆ«æå‡è‡³ CRITICALï¼Œé¿å…æ‰“å°å‡ ç™¾è¡Œ Error
logging.getLogger("telegram.ext.Updater").setLevel(logging.CRITICAL)

# ğŸŒŸ æ–°å¢å¼•å…¥è§†é¢‘æŠ•ç¨¿ä¸­æ¢
from Bot_Publisher.bili_video_uploader import upload_video_bilibili 

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("GloBot_Main")

DATA_DIR = Path(os.getenv("LOCAL_DATA_DIR", f"./GloBot_Data/{settings.targets.group_name}"))
RAW_DIR = DATA_DIR / "timeline_raw"
HISTORY_FILE = DATA_DIR / "history.json"
DYN_MAP_FILE = DATA_DIR / "dyn_map.json"
FIRST_RUN_FLAG_FILE = DATA_DIR / ".first_run_completed"

def load_history():
    if not HISTORY_FILE.exists(): return set()
    try:
        with open(HISTORY_FILE, "r", encoding="utf-8") as f: return set(json.load(f))
    except: return set()

def save_history(history_set):
    HISTORY_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(list(history_set), f, ensure_ascii=False, indent=2)

def load_dyn_map():
    if not DYN_MAP_FILE.exists(): return {}
    try:
        with open(DYN_MAP_FILE, "r", encoding="utf-8") as f: return json.load(f)
    except: return {}

def save_dyn_map(dyn_map):
    DYN_MAP_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(DYN_MAP_FILE, "w", encoding="utf-8") as f:
        json.dump(dyn_map, f, ensure_ascii=False, indent=2)

# ==========================================
# ğŸ§¹ è‡ªåŠ¨åŒ–åª’ä½“åƒåœ¾å›æ”¶æœºåˆ¶
# ==========================================
def cleanup_old_media(retention_days=2.0):
    """å®šæœŸæ¸…ç†è¿‡æœŸçš„åŸå§‹åª’ä½“æ–‡ä»¶ï¼Œé˜²æ­¢ç¡¬ç›˜çˆ†ç‚¸"""
    media_dir = DATA_DIR / "media"
    if not media_dir.exists(): return
    
    current_time = time.time()
    cutoff_time = current_time - (retention_days * 24 * 3600)
    
    deleted_files = 0
    for file_path in media_dir.rglob('*'):
        if file_path.is_file():
            if file_path.stat().st_mtime < cutoff_time:
                try:
                    file_path.unlink()
                    deleted_files += 1
                except Exception as e:
                    logger.error(f"âŒ æ— æ³•åˆ é™¤è¿‡æœŸæ–‡ä»¶ {file_path.name}: {e}")
                    
    # é¡ºæ‰‹æ¸…ç†ç©ºæ–‡ä»¶å¤¹
    for member_dir in media_dir.iterdir():
        if member_dir.is_dir() and not any(member_dir.iterdir()):
            try: member_dir.rmdir()
            except: pass
            
    if deleted_files > 0:
        logger.info(f"ğŸ§¹ [ç©ºé—´ç®¡ç†] è§¦å‘è‡ªåŠ¨æ¸…ç†ï¼å·²æ°¸ä¹…é”€æ¯ {deleted_files} ä¸ªè¶…è¿‡ {retention_days} å¤©çš„é™ˆæ—§åª’ä½“æ–‡ä»¶ã€‚")

# å¤„ç†åª’ä½“ç®¡çº¿çš„å…±ç”¨å‡½æ•°
async def process_media_files(media_list):
    final_paths = []
    video_type = "none"
    for mf in media_list:
        if str(mf).lower().endswith(('.mp4', '.mov')):
            logger.info(f"   -> æ­£åœ¨å¯åŠ¨åª’ä½“ç®¡çº¿å‹åˆ¶è§†é¢‘...")
            source_file = Path(mf)
            PUBLISH_DIR = DATA_DIR / "ready_to_publish"
            PUBLISH_DIR.mkdir(parents=True, exist_ok=True)
            output_file = PUBLISH_DIR / f"final_{source_file.name}"
            
            await dispatch_media(str(source_file))
            if output_file.exists():
                final_paths.append(str(output_file))
                video_type = "translated" if settings.media_engine.enable_ai_translation else "original"
            else:
                final_paths.append(str(source_file)) 
        else:
            final_paths.append(mf)
    return final_paths, video_type

# æ¸…ç†å‹åˆ¶äº§ç‰©
def cleanup_media(media_paths):
    for f in media_paths:
        if "ready_to_publish" in str(f):
            try: Path(f).unlink()
            except: pass

async def process_pipeline(tweet: dict, dyn_map: dict) -> tuple[bool, str]:
    logger.info(f"\n" + "="*50)
    logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ¨æ–‡æ ‘... ç›®æ ‡ç»ˆç‚¹æˆå‘˜: @{tweet['author']}")
    
    # ğŸš¨ è·å– ID ä¿ç•™é…ç½®ç­–ç•¥ (å…œåº•ä¸º 0: å…¨ä¿ç•™)
    id_retention_level = getattr(settings.publishers.bilibili, 'tweet_id_retention', 0)
    prev_dyn_id = None
    
    # ==========================================
    # ğŸ”— ç¬¬ä¸€é˜¶æ®µï¼šä»æ ¹åˆ°å¶ï¼Œå±‚å±‚ç©¿é€å‘å¸ƒå¤–éƒ¨å¼•ç”¨èŠ‚ç‚¹
    # ==========================================
    for ancestor in tweet.get('quote_chain', []):
        anc_id = ancestor['id']
        is_reply = ancestor.get('is_reply', False)
        is_placeholder = ancestor.get('is_placeholder', False)
        
        if anc_id in dyn_map:
            prev_dyn_id = dyn_map[anc_id]
            logger.info(f"   -> â™»ï¸ è®°å¿†å¯»å€å‘½ä¸­ï¼šèŠ‚ç‚¹ {anc_id} å·²æ¬è¿è¿‡ï¼Œè·³è¿‡ã€‚")
            continue
            
        if is_placeholder:
            logger.info(f"   -> âš ï¸ èŠ‚ç‚¹ {anc_id} ä»…ä¸ºé˜²æ–­é“¾å ä½ç¬¦ä¸”æ— è®°å¿†ï¼Œè·³è¿‡å¼ºè¡Œå‘å¸ƒã€‚")
            continue
            
        logger.info(f"   -> â›“ï¸ å‘ç°å…¨æ–°æœªæ¬è¿çš„ç¥–å…ˆèŠ‚ç‚¹ï¼å¼€å§‹ç©¿é€å‘å¸ƒ: @{ancestor['author']}")
        
        anc_translated = await translate_text(ancestor['text'])
        
        dt_str = datetime.fromtimestamp(ancestor['timestamp']).strftime("%Y-%m-%d %H:%M:%S")
        clean_raw = html.unescape(ancestor['text'])
        
        author_handle = ancestor['author']
        author_display = ancestor.get('author_display_name', f"@{author_handle}")
        
        # ğŸ‘‡ æ ¸å¿ƒä¿®å¤ï¼šæ‹¦æˆªéæˆå‘˜ï¼Œå¹¶å¼ºåˆ¶é˜»æ–­å…¨å±€å˜é‡çŠ¶æ€æ±¡æŸ“
        anc_title = settings.targets.account_title_map.get(author_handle, "")
        display_name = anc_title if anc_title else author_display

        # ğŸš¨ æ’ç‰ˆæ¸²æŸ“åˆ†æ”¯ï¼šæç®€èŠå¤©æ°”æ³¡ vs ä¼ ç»Ÿæ’ç‰ˆ
        if is_reply:
            settings.publishers.bilibili.title = "" # å›å¤ä¸åŠ å¡ç‰‡ç‹¬ç«‹æ ‡é¢˜
            anc_content = f"ğŸ’¬ã€{display_name}ã€‘å›å¤è¯´ï¼š\n{anc_translated}\n\n(åŸæ–‡: {clean_raw})"
            if id_retention_level == 0:
                anc_content += f"\n\n{anc_id}"
        else:
            settings.publishers.bilibili.title = anc_title[:15] if anc_title else ""
            header = f"ã€{anc_title}ã€‘\n\n" if anc_title else f"{display_name}\n\n"
            anc_content = f"{header}{dt_str}\n\n{anc_translated}\n\nã€åŸæ–‡ã€‘\n{clean_raw}"
            if id_retention_level < 3:
                anc_content += f"\n\n{anc_id}\n-ç”±GloBoté©±åŠ¨"
        
        anc_content = sanitize_for_bilibili(anc_content)
        
        anc_media, anc_video_type = await process_media_files(ancestor['media'])
        anc_source_url = f"https://x.com/{ancestor['author']}/status/{anc_id}"
        
        has_anc_video = (anc_video_type == "translated" and settings.publishers.bilibili.publish_translated_video) or \
                        (anc_video_type == "original" and settings.publishers.bilibili.publish_original_video)
        vid_path = next((p for p in anc_media if str(p).lower().endswith('.mp4')), None) if has_anc_video else None
        has_any_media = len(anc_media) > 0 # ğŸš¨ è‹¥åŒ…å«ä»»ä½•å›¾/è§†é¢‘ï¼Œç»å¯¹ç¦æ­¢ä½¿ç”¨è½¬å‘å¡ç‰‡ï¼
        
        if prev_dyn_id:
            real_prev_dyn_id = prev_dyn_id
            
            # åŠ¨æ€çŒçŠ¬åæŸ¥ BV å·
            if str(prev_dyn_id).startswith("BV"):
                resolved_id = await get_dynamic_id_by_bvid(prev_dyn_id)
                if resolved_id:
                    real_prev_dyn_id = resolved_id
                    logger.info(f"   -> ğŸ¯ [åŠ¨æ€çŒçŠ¬] æˆåŠŸå°† {prev_dyn_id} åæŸ¥ä¸ºåŠ¨æ€ ID: {real_prev_dyn_id}")
                else:
                    logger.warning(f"   -> âš ï¸ [åŠ¨æ€çŒçŠ¬] åæŸ¥ {prev_dyn_id} å¤±è´¥ï¼Œå°†è¢«è¿«æ‰§è¡Œé™çº§å‘å¸ƒã€‚")

            # ğŸš¨ æ™ºèƒ½é™çº§ä¸åæŸ¥è·¯ç”±
            if has_any_media or str(real_prev_dyn_id).startswith("BV"):
                ref_link = f"https://www.bilibili.com/video/{prev_dyn_id}" if str(prev_dyn_id).startswith("BV") else f"https://t.bilibili.com/{prev_dyn_id}"
                
                # ä¼˜é›…æ‹¼æ¥ä¸Šä¸‹æ–‡æº¯æºé“¾æ¥
                if is_reply:
                    anc_content += f"\n\n(ğŸ”— æº¯æºé“¾æ¥: {ref_link})"
                else:
                    anc_content += f"\n\nğŸ”— æº¯æºé“¾æ¥: {ref_link}"
                    
                if vid_path:
                    logger.info(f"   -> ğŸ†• [æ™ºèƒ½é™çº§] å«åª’ä½“/åæŸ¥æ‹¦æˆªï¼Œè½¬ä¸ºç‹¬ç«‹è§†é¢‘æŠ•ç¨¿ (é™„æº¯æº)...")
                    if id_retention_level >= 2:
                        anc_content = anc_content.replace(f"\n\n{anc_id}\n-ç”±GloBoté©±åŠ¨", "").replace(f"\n\n{anc_id}", "")
                    success, new_anc_dyn_id = await upload_video_bilibili(vid_path, anc_title if anc_title else "æœ€æ–°æ¬è¿", anc_content, anc_source_url, settings)
                else:
                    logger.info(f"   -> ğŸ†• [æ™ºèƒ½é™çº§] å«åª’ä½“/åæŸ¥æ‹¦æˆªï¼Œè½¬ä¸ºç‹¬ç«‹å›¾æ–‡åŠ¨æ€ (é™„æº¯æº)...")
                    success, new_anc_dyn_id = await smart_publish(anc_content, anc_media, video_type=anc_video_type)
            else:
                logger.info(f"   -> ğŸ”„ è§¦å‘ B ç«™æ— é™å¥—å¨ƒæœºåˆ¶...")
                success, new_anc_dyn_id = await smart_repost(anc_content, real_prev_dyn_id)
        else:
            # ğŸ¥ ç¥–å…ˆèŠ‚ç‚¹çš„è§†é¢‘å‘å°„è·¯ç”±
            if vid_path:
                logger.info(f"   -> ğŸ†• [ç¥–å…ˆèŠ‚ç‚¹] ç§»äº¤è§†é¢‘æŠ•ç¨¿ä¸­æ¢...")
                if id_retention_level >= 2:
                    anc_content = anc_content.replace(f"\n\n{anc_id}\n-ç”±GloBoté©±åŠ¨", "").replace(f"\n\n{anc_id}", "")
                success, new_anc_dyn_id = await upload_video_bilibili(vid_path, anc_title if anc_title else "æœ€æ–°æ¬è¿", anc_content, anc_source_url, settings)
            else:
                logger.info(f"   -> ğŸ†• [ç¥–å…ˆèŠ‚ç‚¹] ç§»äº¤å›¾æ–‡é¦–å‘ä¸­æ¢ (é™çº§å¤„ç†)...")
                success, new_anc_dyn_id = await smart_publish(anc_content, anc_media, video_type=anc_video_type)
            
        cleanup_media(anc_media)
        
        if success and new_anc_dyn_id:
            dyn_map[anc_id] = new_anc_dyn_id
            save_dyn_map(dyn_map)
            prev_dyn_id = new_anc_dyn_id
            logger.warning("   -> â³ [é£æ§è§„é¿] ç¥–å…ˆèŠ‚ç‚¹å‘å°„æˆåŠŸï¼Œå¼ºåˆ¶å¼€å¯ 65 ç§’å†·å´é€šé“...")
            await asyncio.sleep(65)
        else:
            logger.error(f"âŒ å¼•ç”¨èŠ‚ç‚¹é“¾æ¡æ–­è£‚ï¼Œå‘å¸ƒç»ˆæ­¢ï¼")
            return False, ""

    # ==========================================
    # ğŸ‘‘ ç¬¬äºŒé˜¶æ®µï¼šå¤„ç†æˆå‘˜çš„æœ€ç»ˆç‚¹è¯„ (å¶å­èŠ‚ç‚¹)
    # ==========================================
    logger.info(f"   -> ğŸ‘‘ é“¾è·¯ç©¿é€å®Œæˆï¼Œå¼€å§‹å¤„ç†æœ€ç»ˆæˆå‘˜ç‚¹è¯„ï¼")
    translated_text = await translate_text(tweet['text'])
    
    raw_title = settings.targets.account_title_map.get(tweet['author'], f"@{tweet['author']}")
    dt_str = datetime.fromtimestamp(tweet['timestamp']).strftime("%Y-%m-%d %H:%M:%S")
    clean_raw_text = html.unescape(tweet['text'])
    
    author_handle = tweet['author']
    display_name = raw_title if author_handle in settings.targets.account_title_map else tweet.get('author_display_name', f"@{author_handle}")
    is_leaf_reply = tweet.get('is_reply', False)

    if is_leaf_reply:
        settings.publishers.bilibili.title = ""
        final_content = f"ğŸ’¬ã€{display_name}ã€‘å›å¤è¯´ï¼š\n{translated_text}\n\n(åŸæ–‡: {clean_raw_text})"
        if id_retention_level == 0:
            final_content += f"\n\n{tweet['id']}"
    else:
        settings.publishers.bilibili.title = raw_title[:15]
        header = f"ã€{raw_title}ã€‘\n\n" if author_handle in settings.targets.account_title_map else ""
        final_content = f"{header}{dt_str}\n\n{translated_text}\n\nã€åŸæ–‡ã€‘\n{clean_raw_text}"
        if id_retention_level < 3:
            final_content += f"\n\n{tweet['id']}\n-ç”±GloBoté©±åŠ¨"

    final_content = sanitize_for_bilibili(final_content)
    
    final_media, video_type = await process_media_files(tweet['media'])
    final_source_url = f"https://x.com/{tweet['author']}/status/{tweet['id']}"
    
    has_final_video = (video_type == "translated" and settings.publishers.bilibili.publish_translated_video) or \
                      (video_type == "original" and settings.publishers.bilibili.publish_original_video)
    vid_path = next((p for p in final_media if str(p).lower().endswith('.mp4')), None) if has_final_video else None
    has_any_media = len(final_media) > 0 # ğŸš¨ ä»»ä½•åª’ä½“éƒ½ä¸èƒ½è¿›åŸç”Ÿè½¬å‘å¡ç‰‡

    if prev_dyn_id:
        real_prev_dyn_id = prev_dyn_id
        
        # åŠ¨æ€çŒçŠ¬åæŸ¥ BV å·
        if str(prev_dyn_id).startswith("BV"):
            resolved_id = await get_dynamic_id_by_bvid(prev_dyn_id)
            if resolved_id:
                real_prev_dyn_id = resolved_id
                logger.info(f"   -> ğŸ¯ [åŠ¨æ€çŒçŠ¬] æˆåŠŸå°† {prev_dyn_id} åæŸ¥ä¸ºåŠ¨æ€ ID: {real_prev_dyn_id}")
            else:
                logger.warning(f"   -> âš ï¸ [åŠ¨æ€çŒçŠ¬] åæŸ¥ {prev_dyn_id} å¤±è´¥ï¼Œå°†è¢«è¿«æ‰§è¡Œé™çº§å‘å¸ƒã€‚")

        if has_any_media or str(real_prev_dyn_id).startswith("BV"):
            ref_link = f"https://www.bilibili.com/video/{prev_dyn_id}" if str(prev_dyn_id).startswith("BV") else f"https://t.bilibili.com/{prev_dyn_id}"
            
            if is_leaf_reply:
                final_content += f"\n\n(ğŸ”— æº¯æºé“¾æ¥: {ref_link})"
            else:
                final_content += f"\n\nğŸ”— æº¯æºé“¾æ¥: {ref_link}"
                
            if vid_path:
                logger.info("   -> ğŸ†• [æ™ºèƒ½é™çº§] æ— æ³•è·¨ç«¯è½¬å‘/åŒ…å«è§†é¢‘ï¼Œè½¬ä¸ºç‹¬ç«‹è§†é¢‘æŠ•ç¨¿ (é™„æº¯æº)...")
                if id_retention_level >= 2:
                    final_content = final_content.replace(f"\n\n{tweet['id']}\n-ç”±GloBoté©±åŠ¨", "").replace(f"\n\n{tweet['id']}", "")
                success, new_dyn_id = await upload_video_bilibili(vid_path, raw_title[:80] if not is_leaf_reply else f"{display_name}çš„è§†é¢‘å›å¤", final_content, final_source_url, settings)
            else:
                logger.info("   -> ğŸ†• [æ™ºèƒ½é™çº§] æºå¤´ä¸ºè§†é¢‘/åŒ…å«åª’ä½“ï¼Œè½¬ä¸ºç‹¬ç«‹å›¾æ–‡åŠ¨æ€ (é™„è§†é¢‘é“¾æ¥)...")
                success, new_dyn_id = await smart_publish(final_content, final_media, video_type=video_type)
        else:
            logger.info(f"   -> â™»ï¸ è§¦å‘æˆå‘˜è½¬å‘åŠ¨ä½œ...")
            success, new_dyn_id = await smart_repost(final_content, real_prev_dyn_id)
    else:
        # ğŸ¥ å¶å­èŠ‚ç‚¹çš„è§†é¢‘å‘å°„è·¯ç”±
        if vid_path:
            logger.info("   -> ç§»äº¤è§†é¢‘æŠ•ç¨¿ä¸­æ¢...")
            if id_retention_level >= 2:
                final_content = final_content.replace(f"\n\n{tweet['id']}\n-ç”±GloBoté©±åŠ¨", "").replace(f"\n\n{tweet['id']}", "")
            success, new_dyn_id = await upload_video_bilibili(vid_path, raw_title[:80] if not is_leaf_reply else f"{display_name}çš„è§†é¢‘å›å¤", final_content, final_source_url, settings)
        else:
            logger.info("   -> ç§»äº¤å›¾æ–‡é¦–å‘ä¸­æ¢ (é™çº§å¤„ç†)...")
            success, new_dyn_id = await smart_publish(final_content, final_media, video_type=video_type)
        
    cleanup_media(final_media)
    return success, new_dyn_id

async def pipeline_loop():
    logger.info("ğŸ¤– GloBot å·¥ä¸šæµæ°´çº¿å·²å¯åŠ¨...")
    
    # è¿™é‡Œçš„ start_telegram_bot å°†æ¥ç®¡å…¨å±€æ§åˆ¶æƒ
    await start_telegram_bot()
    
    is_first_run = not FIRST_RUN_FLAG_FILE.exists()
    history_set = load_history()
    dyn_map = load_dyn_map()
    last_cleanup_time = 0
    
    if is_first_run: logger.warning("ğŸš¨ æ£€æµ‹åˆ°é¦–æ¬¡éƒ¨ç½²ï¼é¦–å‘æˆªæ–­ä¿æŠ¤æœºåˆ¶å·²å°±ç»ªã€‚")
    
    while True:
        try:
            # ğŸ‘‡ 2. é˜€é—¨å¡å£ï¼šå¦‚æœ TG ä¸‹è¾¾äº†æš‚åœæŒ‡ä»¤ï¼Œè¿™é‡Œä¼šæ— é™æŒ‚èµ·ï¼Œç›´åˆ°æ¢å¤
            await GloBotState.is_running.wait()

            if time.time() - last_cleanup_time > 12 * 3600:
                retention = getattr(settings.system, 'media_retention_days', 2.0)
                cleanup_old_media(retention_days=retention)
                last_cleanup_time = time.time()

            logger.info("\nğŸ“¡ å¯åŠ¨çˆ¬è™«å—…æ¢...")
            await fetch_timeline()
            
            json_files = list(RAW_DIR.glob("*.json"))
            if not json_files:
                # ğŸ‘‡ æ‰¾å›è¿™è¡Œæ—¥å¿—
                logger.info("ğŸ’¤ æœªå‘ç° JSON çŸ¿çŸ³ï¼Œä¼‘çœ  60 ç§’...")
                GloBotState.is_sleeping = True
                GloBotState.wake_up_event.clear()
                try:
                    await asyncio.wait_for(GloBotState.wake_up_event.wait(), timeout=60)
                    logger.info("âš¡ æ”¶åˆ°å¼ºåˆ¶å”¤é†’ä¿¡å·ï¼Œæå‰ç»“æŸä¼‘çœ ï¼")
                except asyncio.TimeoutError:
                    pass
                finally:
                    GloBotState.is_sleeping = False
                continue
                
            latest_json = max(json_files, key=os.path.getmtime)
            new_tweets = await parse_timeline_json(latest_json)
            
            for jf in json_files:
                if jf.name != latest_json.name:
                    try: jf.unlink()
                    except: pass
            
            if not new_tweets:
                sleep_time = random.randint(240, 420)
                # ğŸ‘‡ æ‰¾å›è¿™è¡Œæ—¥å¿—
                logger.info(f"ğŸ’¤ æ— æ–°åŠ¨æ€ï¼Œä¼‘çœ  {sleep_time} ç§’...")
                GloBotState.is_sleeping = True
                GloBotState.wake_up_event.clear()
                try:
                    await asyncio.wait_for(GloBotState.wake_up_event.wait(), timeout=sleep_time)
                    logger.info("âš¡ æ”¶åˆ°å¼ºåˆ¶å”¤é†’ä¿¡å·ï¼Œæå‰ç»“æŸä¼‘çœ ï¼")
                except asyncio.TimeoutError:
                    pass
                finally:
                    GloBotState.is_sleeping = False
                continue
                
            new_tweets.sort(key=lambda x: x['timestamp'])
            
            if is_first_run:
                for t in new_tweets[:-1]: history_set.add(str(t['id']))
                save_history(history_set)
                new_tweets = [new_tweets[-1]]
                FIRST_RUN_FLAG_FILE.touch()
                is_first_run = False

            total = len(new_tweets)
            for i, tweet in enumerate(new_tweets):
                # ğŸ‘‡ æ¯æ¬¡å‘æ¨å‰éƒ½æ£€æŸ¥ä¸€ä¸‹é˜€é—¨çŠ¶æ€
                await GloBotState.is_running.wait()
                
                tweet_id = str(tweet['id'])
                
                try:
                    success, new_dyn_id = await process_pipeline(tweet, dyn_map)
                    
                    if success:
                        history_set.add(tweet_id)
                        save_history(history_set)
                        if new_dyn_id:
                            dyn_map[tweet_id] = new_dyn_id
                            save_dyn_map(dyn_map)
                        logger.info(f"âœ… ä»»åŠ¡ {i+1}/{total} [{tweet_id}] æˆåŠŸå‘å°„ï¼")
                        GloBotState.daily_stats['success'] += 1  # ç»Ÿè®¡æˆåŠŸ
                        
                        # ğŸŒŸ æ–°å¢ï¼šæ•´ä½“æµç¨‹æ’­æŠ¥ (é’ˆå¯¹å›¾æ–‡æˆ–æ™®é€šæ¬è¿)
                        if not str(new_dyn_id).startswith("BV"):  # è§†é¢‘ä¼šåœ¨ä¸“é—¨çš„æ¨¡å—å‘æ¨é€ï¼Œè¿™é‡Œè¿‡æ»¤æ‰ä»¥é˜²é‡å¤
                            await send_tg_msg(f"ğŸ‰ <b>å›¾æ–‡æ¬è¿æˆåŠŸ</b> [{i+1}/{total}]\næ¨ç‰¹æº: <code>{tweet_id}</code>\næˆåŠŸç”Ÿæˆ Bç«™åŠ¨æ€: <code>{new_dyn_id}</code>")

                    else:
                        logger.error(f"âŒ æ¨æ–‡ {tweet_id} å‘å¸ƒå¤±è´¥ï¼")
                        GloBotState.daily_stats['failed'] += 1   # ç»Ÿè®¡å¤±è´¥
                        
                        # ğŸŒŸ æ–°å¢ï¼šå¤±è´¥æ€»ä½“æç¤º
                        await send_tg_msg(f"âŒ <b>æ¬è¿å—é˜»</b> [{i+1}/{total}]\næ¨ç‰¹æº: <code>{tweet_id}</code>\næœªèƒ½æˆåŠŸå‘å¸ƒï¼Œè¯·æ£€æŸ¥ç»ˆç«¯æ—¥å¿—æ’æŸ¥ã€‚")
                        continue
                        
                except RuntimeError as e: # ğŸš¨ æ ¸å¿ƒæ”¹åŠ¨ï¼šåŠ å…¥å…¨å±€å®‰å…¨ç†”æ–­å™¨ï¼
                    if "AUTH_EXPIRED" in str(e):
                        logger.critical(f"ğŸ›‘ [ç†”æ–­æœºåˆ¶] ä¾¦æµ‹åˆ°å‡­è¯å¤±æ•ˆï¼Œå¼ºè¡Œåˆ‡æ–­æµæ°´çº¿: {e}")
                        GloBotState.is_running.clear() # ç‰©ç†é”æ­»æ€»çº¿
                        await send_tg_error(f"ğŸ›‘ <b>å®‰å…¨ç†”æ–­æœºåˆ¶è§¦å‘ï¼</b>\n\næ£€æµ‹åˆ°è´¦å·ä»¤ç‰Œå¤±æ•ˆæˆ–è¢«æ‹¦æˆªï¼š\n<code>{e}</code>\n\nä¸ºé˜²æ­¢æ— é™é‡è¯•å¯¼è‡´æ­»å°ï¼Œæµæ°´çº¿å·²<b>å¼ºåˆ¶ç‰©ç†æŒ‚èµ·</b>ã€‚\nğŸ‘‰ è¯·åœ¨ç»ˆç«¯è¿è¡Œ `python Bot_Publisher/bili_login.py` é‡æ–°æ‰«ç ï¼Œæ›´æ–°å‡­è¯åå‘é€ <code>/resume</code> æ¢å¤è¿è¡Œã€‚")
                        break # å¼ºåˆ¶è·³å‡ºè¿™æ‰¹æ¨æ–‡çš„å¾ªç¯ï¼Œè¿›å…¥æœ€å¤–å±‚çš„ wait() æŒ‚èµ·ç­‰å¾…
                    else:
                        err_trace = traceback.format_exc()
                        logger.error(f"ğŸ”¥ å¤„ç†æ¨æ–‡ {tweet_id} æ—¶å‘ç”Ÿè¿è¡Œæ—¶å¼‚å¸¸: {e}")
                        await send_tg_error(f"å¤„ç†æ¨æ–‡å´©æºƒ:\n{err_trace[-300:]}")
                        GloBotState.daily_stats['failed'] += 1
                        continue

                except Exception as e:
                    err_trace = traceback.format_exc()
                    logger.error(f"ğŸ”¥ å¤„ç†æ¨æ–‡ {tweet_id} æ—¶å‘ç”Ÿå†…éƒ¨å´©æºƒ: {e}")
                    # ğŸ‘‡ 3. æŠ›å‡ºè‡´å‘½å¼‚å¸¸åˆ°ä¸»ç†äººçš„æ‰‹æœºä¸Šï¼
                    await send_tg_error(f"å¤„ç†æ¨æ–‡ {tweet_id} å´©æºƒ:\n{err_trace[-300:]}")
                    GloBotState.daily_stats['failed'] += 1
                    continue
                    
                if i < total - 1:
                        logger.warning("â³ [é£æ§è§„é¿] å•ä¸ªæˆå‘˜ä»»åŠ¡å®Œæˆï¼Œä¼‘çœ  65 ç§’è¿›å…¥ä¸‹ä¸€ä»»åŠ¡...")
                        await asyncio.sleep(65)
                    
            sleep_time = random.randint(240, 420)
            # ğŸ‘‡ æ‰¾å›è¿™è¡Œæ—¥å¿—
            logger.info(f"âœ… å‘¨æœŸå·¡è§†å®Œæˆï¼Œæ·±åº¦ä¼‘çœ  {sleep_time} ç§’...")
            GloBotState.is_sleeping = True
            GloBotState.wake_up_event.clear()
            try:
                await asyncio.wait_for(GloBotState.wake_up_event.wait(), timeout=sleep_time)
                logger.info("âš¡ æ”¶åˆ°å¼ºåˆ¶å”¤é†’ä¿¡å·ï¼Œæå‰ç»“æŸä¼‘çœ ï¼")
            except asyncio.TimeoutError:
                pass
            finally:
                GloBotState.is_sleeping = False
            
        except Exception as e:
            err_trace = traceback.format_exc()
            logger.error(f"ğŸ”¥ æ€»çº¿å‘ç”Ÿæœªæ•è·å¼‚å¸¸: {e}")
            # ğŸ‘‡ å°†æ€»çº¿çº§å´©æºƒç›´æ¥æ¨é€åˆ° Telegram
            await send_tg_error(f"æ€»çº¿æŒ‚æœºå¤§å´©æºƒ:\n{err_trace[-400:]}")
            await asyncio.sleep(60)

async def main_master():
    logger.info("ğŸ¤– åˆå§‹åŒ– Telegram ä¸­æ¢...")
    GloBotState.main_loop_coro = pipeline_loop
    await start_telegram_bot()
    GloBotState.crawler_task = asyncio.create_task(pipeline_loop())
    await send_tg_msg("ğŸŸ¢ <b>GloBot Matrix å·²ä¸Šçº¿</b>\næ€»çº¿è¿æ¥æ­£å¸¸ï¼Œé»˜è®¤æµæ°´çº¿å·²è‡ªåŠ¨ç‚¹ç«ã€‚æ‚¨å¯éšæ—¶é€šè¿‡ <code>/kill</code> å…³åœã€‚")
    logger.info("ğŸŸ¢ GloBot ä¸»æ§èŠ‚ç‚¹å·²å°±ç»ªï¼Œæ­£åœ¨æ°¸ä¹…æŒ‚èµ·ä¸»çº¿ç¨‹ç›‘å¬æŒ‡ä»¤...")
    while True:
        await asyncio.sleep(86400)

if __name__ == "__main__":
    try: asyncio.run(main_master())
    except KeyboardInterrupt: logger.info("\nğŸ›‘ æ”¶åˆ°ä¸»æ§å°åˆ‡æ–­ä¿¡å·ï¼ŒGloBot å®‰å…¨åœæœºã€‚")