import os
import json
import time
import logging
import asyncio
import random
import html
from pathlib import Path
from datetime import datetime

from common.config_loader import settings
from Bot_Crawler.twitter_scraper import fetch_timeline
from Bot_Crawler.tweet_parser import parse_timeline_json
from Bot_Media.llm_translator import translate_text
from Bot_Media.media_pipeline import dispatch_media
from Bot_Publisher.bili_uploader import smart_publish, smart_repost

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("GloBot_Main")

DATA_DIR = Path(os.getenv("LOCAL_DATA_DIR", f"./GloBot_Data/{settings.targets.group_name}"))
RAW_DIR = DATA_DIR / "timeline_raw"
HISTORY_FILE = DATA_DIR / "history.json"
DYN_MAP_FILE = DATA_DIR / "dyn_map.json"
FIRST_RUN_FLAG_FILE = DATA_DIR / ".first_run_completed"

def load_history():
    if not HISTORY_FILE.exists(): return set()
    try:
        with open(HISTORY_FILE, "r", encoding="utf-8") as f: return set(json.load(f))
    except: return set()

def save_history(history_set):
    HISTORY_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(list(history_set), f, ensure_ascii=False, indent=2)

def load_dyn_map():
    if not DYN_MAP_FILE.exists(): return {}
    try:
        with open(DYN_MAP_FILE, "r", encoding="utf-8") as f: return json.load(f)
    except: return {}

def save_dyn_map(dyn_map):
    DYN_MAP_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(DYN_MAP_FILE, "w", encoding="utf-8") as f:
        json.dump(dyn_map, f, ensure_ascii=False, indent=2)

# å¤„ç†åª’ä½“ç®¡çº¿çš„å…±ç”¨å‡½æ•°
async def process_media_files(media_list):
    final_paths = []
    video_type = "none"
    for mf in media_list:
        if str(mf).lower().endswith(('.mp4', '.mov')):
            logger.info(f"   -> æ­£åœ¨å¯åŠ¨åª’ä½“ç®¡çº¿å‹åˆ¶è§†é¢‘...")
            source_file = Path(mf)
            PUBLISH_DIR = DATA_DIR / "ready_to_publish"
            PUBLISH_DIR.mkdir(parents=True, exist_ok=True)
            output_file = PUBLISH_DIR / f"final_{source_file.name}"
            
            await dispatch_media(str(source_file))
            if output_file.exists():
                final_paths.append(str(output_file))
                video_type = "translated" if settings.media_engine.enable_ai_translation else "original"
            else:
                final_paths.append(str(source_file)) 
        else:
            final_paths.append(mf)
    return final_paths, video_type

# æ¸…ç†å‹åˆ¶äº§ç‰©
def cleanup_media(media_paths):
    for f in media_paths:
        if "ready_to_publish" in str(f):
            try: Path(f).unlink()
            except: pass

async def process_pipeline(tweet: dict, dyn_map: dict) -> tuple[bool, str]:
    logger.info(f"\n" + "="*50)
    logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ¨æ–‡æ ‘... ç›®æ ‡ç»ˆç‚¹æˆå‘˜: @{tweet['author']}")
    
    prev_dyn_id = None
    
    # ==========================================
    # ğŸ”— ç¬¬ä¸€é˜¶æ®µï¼šä»æ ¹åˆ°å¶ï¼Œå±‚å±‚ç©¿é€å‘å¸ƒå¤–éƒ¨å¼•ç”¨èŠ‚ç‚¹
    # ==========================================
    for ancestor in tweet.get('quote_chain', []):
        anc_id = ancestor['id']
        
        # å¦‚æœè¿™ä¸ªè€ç¥–å®—å·²ç»å‘è¿‡ B ç«™äº†ï¼Œç›´æ¥ç»§æ‰¿å®ƒçš„ IDï¼Œç»§ç»­å¾€ä¸‹èµ°
        if anc_id in dyn_map:
            prev_dyn_id = dyn_map[anc_id]
            logger.info(f"   -> â™»ï¸ è®°å¿†å¯»å€å‘½ä¸­ï¼šèŠ‚ç‚¹ {anc_id} å·²æ¬è¿è¿‡ï¼Œè·³è¿‡ã€‚")
            continue
            
        logger.info(f"   -> â›“ï¸ å‘ç°å…¨æ–°æœªæ¬è¿çš„ç¥–å…ˆèŠ‚ç‚¹ï¼å¼€å§‹ç©¿é€å‘å¸ƒ: @{ancestor['author']}")
        
        # 1. ç¿»è¯‘ç¥–å…ˆèŠ‚ç‚¹
        anc_translated = await translate_text(ancestor['text'])
        
        # 2. å®Œç¾çš„æ’ç‰ˆç»„è£…ï¼ˆæ— è§† B ç«™æ ‡é¢˜ï¼Œç›´æ¥æ‹¼è£…åˆ°å†…å®¹é¡¶éƒ¨ï¼‰
        anc_title = settings.targets.account_title_map.get(ancestor['author'], f"@{ancestor['author']}")
        dt_str = datetime.fromtimestamp(ancestor['timestamp']).strftime("%Y-%m-%d %H:%M:%S")
        clean_raw = html.unescape(ancestor['text'])
        
        anc_content = f"ã€{anc_title}ã€‘\n\n{dt_str}\n\n{anc_translated}\n\nã€åŸæ–‡ã€‘\n{clean_raw}\n\n{anc_id}\n-ç”±GloBoté©±åŠ¨"
        
        # 3. å¤„ç†ç¥–å…ˆåª’ä½“æ–‡ä»¶
        anc_media, anc_video_type = await process_media_files(ancestor['media'])
        
        # 4. å‘å¸ƒï¼ˆåˆ¤æ–­æ˜¯é¦–å‘è¿˜æ˜¯è½¬å‘å¥—å¨ƒï¼‰
        if prev_dyn_id:
            logger.info(f"   -> ğŸ”„ è§¦å‘ B ç«™æ— é™å¥—å¨ƒæœºåˆ¶...")
            success, new_anc_dyn_id = await smart_repost(anc_content, prev_dyn_id)
        else:
            logger.info(f"   -> ğŸ†• æ­£åœ¨å°†æ¨æ–‡æ ‘çš„æœ€åº•å±‚æ ¹èŠ‚ç‚¹è¿›è¡Œé¦–å‘...")
            success, new_anc_dyn_id = await smart_publish(anc_content, anc_media, video_type=anc_video_type)
            
        cleanup_media(anc_media)
        
        # 5. ä¸¥æ ¼é£æ§
        if success and new_anc_dyn_id:
            dyn_map[anc_id] = new_anc_dyn_id
            save_dyn_map(dyn_map)
            prev_dyn_id = new_anc_dyn_id
            logger.warning("   -> â³ [é£æ§è§„é¿] ç¥–å…ˆèŠ‚ç‚¹å‘å°„æˆåŠŸï¼Œå¼ºåˆ¶å¼€å¯ 65 ç§’å†·å´é€šé“...")
            await asyncio.sleep(65)
        else:
            logger.error(f"âŒ å¼•ç”¨èŠ‚ç‚¹é“¾æ¡æ–­è£‚ï¼Œå‘å¸ƒç»ˆæ­¢ï¼")
            return False, ""

    # ==========================================
    # ğŸ‘‘ ç¬¬äºŒé˜¶æ®µï¼šå¤„ç†æˆå‘˜çš„æœ€ç»ˆç‚¹è¯„ (å¶å­èŠ‚ç‚¹)
    # ==========================================
    logger.info(f"   -> ğŸ‘‘ é“¾è·¯ç©¿é€å®Œæˆï¼Œå¼€å§‹å¤„ç†æœ€ç»ˆæˆå‘˜ç‚¹è¯„ï¼")
    translated_text = await translate_text(tweet['text'])
    
    raw_title = settings.targets.account_title_map.get(tweet['author'], f"@{tweet['author']}")
    dt_str = datetime.fromtimestamp(tweet['timestamp']).strftime("%Y-%m-%d %H:%M:%S")
    clean_raw_text = html.unescape(tweet['text'])
    
    final_content = f"{dt_str}\n\n{translated_text}\n\nã€åŸæ–‡ã€‘\n{clean_raw_text}\n\n{tweet['id']}\n-ç”±GloBoté©±åŠ¨"

    # é’ˆå¯¹é¦–å‘åŠ¨æ€çš„å®‰å…¨æ ‡é¢˜ (åªæœ‰ä¸æ˜¯è½¬å‘æ—¶æ‰ä¼šç”¨åˆ°è¿™ä¸ªå­—æ®µ)
    settings.publishers.bilibili.title = raw_title[:15]
    
    final_media, video_type = await process_media_files(tweet['media'])
    
    if prev_dyn_id:
        logger.info(f"   -> â™»ï¸ è§¦å‘æˆå‘˜è½¬å‘åŠ¨ä½œ...")
        success, new_dyn_id = await smart_repost(final_content, prev_dyn_id)
    else:
        logger.info("   -> ç§»äº¤é¦–å‘ä¸­æ¢...")
        success, new_dyn_id = await smart_publish(final_content, final_media, video_type=video_type)
        
    cleanup_media(final_media)
    return success, new_dyn_id


async def main_loop():
    logger.info("ğŸ¤– GloBot å·¥ä¸šæµæ°´çº¿å·²å¯åŠ¨...")
    is_first_run = not FIRST_RUN_FLAG_FILE.exists()
    history_set = load_history()
    dyn_map = load_dyn_map()
    
    if is_first_run: logger.warning("ğŸš¨ æ£€æµ‹åˆ°é¦–æ¬¡éƒ¨ç½²ï¼é¦–å‘æˆªæ–­ä¿æŠ¤æœºåˆ¶å·²å°±ç»ªã€‚")
    
    while True:
        try:
            logger.info("\nğŸ“¡ å¯åŠ¨çˆ¬è™«å—…æ¢...")
            await fetch_timeline()
            
            json_files = list(RAW_DIR.glob("*.json"))
            if not json_files:
                logger.info("ğŸ’¤ æœªå‘ç° JSON çŸ¿çŸ³ï¼Œä¼‘çœ ä¸­...")
                await asyncio.sleep(60)
                continue
                
            latest_json = max(json_files, key=os.path.getmtime)
            new_tweets = await parse_timeline_json(latest_json)
            
            for jf in json_files:
                if jf.name != latest_json.name:
                    try: jf.unlink()
                    except: pass
            
            if not new_tweets:
                sleep_time = random.randint(240, 420)
                logger.info(f"ğŸ’¤ æ— æ–°åŠ¨æ€ï¼Œä¼‘çœ  {sleep_time} ç§’...")
                await asyncio.sleep(sleep_time)
                continue
                
            new_tweets.sort(key=lambda x: x['timestamp'])
            
            if is_first_run:
                logger.warning(f"ğŸš¨ [é¦–å‘ä¿æŠ¤] æ£€æµ‹åˆ°é¦–æ¬¡å¯åŠ¨ï¼Œçˆ¬å–åˆ° {len(new_tweets)} æ¡å†å²æ¨æ–‡ï¼Œä»…ä¿ç•™æœ€æ–°ä¸€æ¡ï¼")
                for t in new_tweets[:-1]: history_set.add(str(t['id']))
                save_history(history_set)
                new_tweets = [new_tweets[-1]]
                FIRST_RUN_FLAG_FILE.touch()
                is_first_run = False
            else:
                logger.info(f"ğŸ¯ å¾…å¤„ç†é˜Ÿåˆ—ï¼š{len(new_tweets)} æ¡åŠ¨æ€")

            total = len(new_tweets)
            for i, tweet in enumerate(new_tweets):
                tweet_id = str(tweet['id'])
                success, new_dyn_id = await process_pipeline(tweet, dyn_map)
                
                if success:
                    history_set.add(tweet_id)
                    save_history(history_set)
                    if new_dyn_id:
                        dyn_map[tweet_id] = new_dyn_id
                        save_dyn_map(dyn_map)
                        
                    logger.info(f"âœ… ä»»åŠ¡ {i+1}/{total} [{tweet_id}] æˆåŠŸå‘å°„ï¼")
                else:
                    logger.error(f"âŒ æ¨æ–‡ {tweet_id} å‘å¸ƒå¤±è´¥ï¼")
                    break
                    
                if i < total - 1:
                    logger.warning("â³ [é£æ§è§„é¿] å•ä¸ªæˆå‘˜ä»»åŠ¡å®Œæˆï¼Œä¼‘çœ  65 ç§’è¿›å…¥ä¸‹ä¸€ä»»åŠ¡...")
                    await asyncio.sleep(65)
                    
            sleep_time = random.randint(240, 420)
            logger.info(f"âœ… å‘¨æœŸå·¡è§†å®Œæˆï¼Œæ·±åº¦ä¼‘çœ  {sleep_time} ç§’...")
            await asyncio.sleep(sleep_time)
            
        except Exception as e:
            logger.error(f"ğŸ”¥ æ€»çº¿å‘ç”Ÿæœªæ•è·å¼‚å¸¸: {e}")
            await asyncio.sleep(60)

if __name__ == "__main__":
    try: asyncio.run(main_loop())
    except KeyboardInterrupt: logger.info("\nğŸ›‘ æ”¶åˆ°ä¸»æ§å°åˆ‡æ–­ä¿¡å·ï¼ŒGloBot å®‰å…¨åœæœºã€‚")